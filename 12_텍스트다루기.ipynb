{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12. 텍스트다루기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDamHlHeT2yHqyMoMSxZEh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckycontrol/DeepLearning_tensorflow/blob/main/12_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%8B%A4%EB%A3%A8%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC4vmoCJ99-Z"
      },
      "source": [
        "# Tokenization(토큰화) 이론\n",
        "\n",
        "텍스트 ( 문장, 문단, 문서 ) 에서 어디 까지가 문장이고 무엇이 단어인지 알려주는 것을 의미한다.\n",
        "\n",
        "- 문장 토큰화\n",
        "- 단어 토큰화\n",
        "- subword 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lTT-ApGAcyu"
      },
      "source": [
        "## English Tokenization\n",
        "\n",
        "띄어쓰기 및 온점을 이용해 단어 및 문장에 대한 토큰화를 쉽게 진행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USfEQE019jix"
      },
      "source": [
        "sample_txt = \"Everywhere I turn, no matter where I look The systems in control, it's all ran by the book. I've got to get away so I can clear my mind, Xscape is what I need, Away from electric eyes.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSw2hzCVBduh"
      },
      "source": [
        "문장 토큰화 (Sentence Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9B1Ra_SBTue",
        "outputId": "4f89b7a9-ebc6-48c7-ec3c-052658841390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_sentece = sample_txt.split(', ')\n",
        "tokenized_sentece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Everywhere I turn',\n",
              " 'no matter where I look The systems in control',\n",
              " \"it's all ran by the book. I've got to get away so I can clear my mind\",\n",
              " 'Xscape is what I need',\n",
              " 'Away from electric eyes.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlBbDcb1CDzI"
      },
      "source": [
        "단어 토큰화 (Word Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNcRIujVBrfI",
        "outputId": "6a6f1450-e3db-4dbf-e553-dec732c49771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_word = sample_txt.split()\n",
        "tokenized_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Everywhere',\n",
              " 'I',\n",
              " 'turn,',\n",
              " 'no',\n",
              " 'matter',\n",
              " 'where',\n",
              " 'I',\n",
              " 'look',\n",
              " 'The',\n",
              " 'systems',\n",
              " 'in',\n",
              " 'control,',\n",
              " \"it's\",\n",
              " 'all',\n",
              " 'ran',\n",
              " 'by',\n",
              " 'the',\n",
              " 'book.',\n",
              " \"I've\",\n",
              " 'got',\n",
              " 'to',\n",
              " 'get',\n",
              " 'away',\n",
              " 'so',\n",
              " 'I',\n",
              " 'can',\n",
              " 'clear',\n",
              " 'my',\n",
              " 'mind,',\n",
              " 'Xscape',\n",
              " 'is',\n",
              " 'what',\n",
              " 'I',\n",
              " 'need,',\n",
              " 'Away',\n",
              " 'from',\n",
              " 'electric',\n",
              " 'eyes.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_QKrDXeC95i"
      },
      "source": [
        "## 띄어쓰기로 영어 문장 내 단어 구분할 때의 문제점\n",
        "* We're Genius!!\n",
        "* We are Genius!\n",
        "* We are Genius\n",
        "\n",
        "우리는 위 세 문장이 모두 같은 의미라는 것을 알지만, 기계는 그렇지 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1qg_b8_Dek8"
      },
      "source": [
        "## 해결방법 첫 번째 - 특수문자 지우기\n",
        "\n",
        "* `[We, re, Genius]`\n",
        "* `[We, are, Genius]`\n",
        "* `[We, are, Genius]`\n",
        "\n",
        "특수문자가 중요한 의미를 가지는 경우에도 특수문자를 삭제하는게 맞을까?\n",
        "* $12.45 -> `[12, 45]`\n",
        "* Mr. So -> `[Mr, So]`\n",
        "* Mrs. Kim -> `[Mrs, Kim]`\n",
        "* 192.168.0.1 -> `[192, 168, 0, 1]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcMGAGbsEz9i"
      },
      "source": [
        "## 영어 단어 토크나이저 활용하기\n",
        "\n",
        "* TreebankWordTokenizer 패키지가 있다.\n",
        "  - 영어 표준 토큰화 규격을 따라간다.\n",
        "  - Fenn Treebank Tokenization 규칙\n",
        "\n",
        "* TreebankWordTokenizer의 규칙\n",
        "  - 하이픈으로 구성된 단어는 하나로 유지\n",
        "  - doesn't 같이 어퍼스트로피로 '접어'가 함께하는 단어는 따로 분리 해준다.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_qKAelzEzqR"
      },
      "source": [
        "## 한국어 토큰화가 어려운 이유!\n",
        "\n",
        "1. 한국어는 교착어이다.\n",
        "2. 한국어는 띄어쓰기가 잘 안지켜진다.\n",
        "3. 한국어는 주어생략이 가능하고 어순도 중요하지 않다.\n",
        "4. 한자어라는 특성상 하나의 음절도 다른 의미를 가질 수 있다.\n",
        "\n",
        "### 교착어\n",
        "실질적인 의미를 가지는 어간에 조사나 어미와 같은 문법 형태소가 결합해서 문법적인 기능(각각 다른 의미를 갖는)이 부여되는 언어\n",
        "ex) 책상 + 은,는,이,가 \n",
        "\n",
        "### 띄어쓰기 문제\n",
        "> 거지같이띄어쓰기를하지않아도읽는데문제가없다.   \n",
        "> Thisishardtoreadyouknow\n",
        "\n",
        "`py-hanspell` 패키지 또는 `ko-spacing` 패키지를 이용해 문법이나 띄어쓰기 교정을 할 수 있다.\n",
        "\n",
        "\n",
        "### 주어 생략 및 어순 문제\n",
        "1. 나는 운동을 했다. 체육관에서.\n",
        "2. 나는 체육관에서 운동을 했다.\n",
        "3. 체육관에서 운동을 했다.\n",
        "4. 나는 운동을 체육관에서 했다.\n",
        "\n",
        "### 한자어특성 때문에 하나의 음절이 다른 의미를 갖는 것\n",
        "1. 배 ( 사람의 배?, 먹는 배?, 타는 배? )\n",
        "2. 한국 ( 한 과 국이 따로 봐도 의미가 있다. )\n",
        "\n",
        "따라서 형태소분석기를 사용한다.\n",
        "\n",
        "한국어의 형태소분석을 쉽게 하기 위해서 Konlpy 패키지 활용\n",
        "```python\n",
        "!pip install konlpy\n",
        "```\n",
        "\n",
        "그 외\n",
        "kai\n",
        "mecab \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6jj7W4BM-_0"
      },
      "source": [
        "### Mecab\n",
        "\n",
        "실무에서 가장 많이 사용되고 있는 형태소 분석기.  \n",
        "```python\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EKb-sARNURI"
      },
      "source": [
        "# Twitter, 꼬꼬마, 코모란, 한나눔\n",
        "속도가 Mecab이나 khaii보다 느리지만 각각 장단점이 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHCI8sCtN93m"
      },
      "source": [
        "## cleaning(정제)와 Normalization(정규화)\n",
        "\n",
        "### 정제란?\n",
        "* 불필요한 데이터를 제거하는 것\n",
        "* 텍스트 중간에 껴있는 숫자나 특수기호를 제거하는 것\n",
        "* 한국어의 경우 은, 는, 이, 가 등의 불용어(stopwords)를 제거할 때 사용\n",
        "* 영어의 경우 at, is, am, the 등을 제거\n",
        "* 텍스트의 인코딩 문제 해결\n",
        "* 길이가 짧은 단어를 제거\n",
        "* 등장 빈도가 적은 단어 제거\n",
        "\n",
        "\n",
        "### 정규화란?\n",
        "* 문장의 복잡도를 줄여주는 과정\n",
        "* 같은 의미를 갖는 여러 단어를 하나로 통합하는 작업\n",
        "* 영어의 경우 lemmatization\n",
        "  * am, are, were, was -> be\n",
        "  * has, had -> have\n",
        "  * 10, 12, 100 -> num\n",
        "  * ㅋ, ㅋㅋㅋㅋ, ㅋㅋㅋㅋㅋ -> ㅋㅋ\n",
        "* 대소문자 통합 등\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvamTyTbPmpa"
      },
      "source": [
        "# Tokenization 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxjpWSgoCOHH",
        "outputId": "0bca07ca-b74f-41d0-fe29-631de05d10e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # 영어 토크나이저 활용하기\n",
        "\n",
        "sentence = \"Ain't nothin' sweeter, you want this sugar, don't ya?\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4qMu51P7KE"
      },
      "source": [
        "## 기본 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNNZGsV5P2Rk",
        "outputId": "0ce8a169-2a10-4169-d7f9-1c6d2fd96ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sentence))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gyP0oydQJul"
      },
      "source": [
        "## WorkPunkTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha_TSS-jQCw2",
        "outputId": "01a9d3d1-1e22-44e7-ed5f-7ddc201f2b6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(sentence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ain', \"'\", 't', 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'don', \"'\", 't', 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lePekM28QdGx"
      },
      "source": [
        "# TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1iaovnbQW1M",
        "outputId": "77e340ed-9029-447a-fda4-3772a4928e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(sentence))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkH29dz2Qz1m",
        "outputId": "51034306-3fa9-4998-f2a9-66462aef4092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_txt = \"I'm Iron-man\"\n",
        "print(tokenizer.tokenize(sample_txt))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'m\", 'Iron-man']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4F_FYw_RFiu"
      },
      "source": [
        "## 한글 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdlEpps7RAfo"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AILWdB_tRJC8"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWho3J2_RQAA"
      },
      "source": [
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt # 트위터\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()\n",
        "\n",
        "sentence = \"밥을 먹고 학교에 간다. 학원에 간다.\"\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHtUEV-fTXMi"
      },
      "source": [
        "# 트위터 (Okt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am0HxGrlTF1F",
        "outputId": "c78f32fb-1115-4c22-ca6d-4c70893f1586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(okt.nouns(sentence)) # 명사만 추출\n",
        "print(okt.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(okt.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['밥', '학교', '간다', '학원', '간다']\n",
            "['밥', '을', '먹고', '학교', '에', '간다', '.', '학원', '에', '간다', '.']\n",
            "[('밥', 'Noun'), ('을', 'Josa'), ('먹고', 'Verb'), ('학교', 'Noun'), ('에', 'Josa'), ('간다', 'Noun'), ('.', 'Punctuation'), ('학원', 'Noun'), ('에', 'Josa'), ('간다', 'Noun'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO29xvCCTwxH"
      },
      "source": [
        "# 꼬꼬마 (Kkma)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66_XfedATmL6",
        "outputId": "fb56a2bd-7e9c-49ff-b2cd-79220be5e78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(kkma.nouns(sentence)) # 명사만 추출\n",
        "print(kkma.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(kkma.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['밥', '학교', '학원']\n",
            "['밥', '을', '먹', '고', '학교', '에', '가', 'ㄴ다', '.', '학원', '에', '가', 'ㄴ다', '.']\n",
            "[('밥', 'NNG'), ('을', 'JKO'), ('먹', 'VV'), ('고', 'ECE'), ('학교', 'NNG'), ('에', 'JKM'), ('가', 'VV'), ('ㄴ다', 'EFN'), ('.', 'SF'), ('학원', 'NNG'), ('에', 'JKM'), ('가', 'VV'), ('ㄴ다', 'EFN'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfqMVJItUB4f"
      },
      "source": [
        "# 코모란 (Komoran)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EySGuNFHT5Wb",
        "outputId": "8441017c-be2a-4aa6-97d0-2367151f6059",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(komoran.nouns(sentence)) # 명사만 추출\n",
        "print(komoran.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(komoran.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['밥', '학교', '학원']\n",
            "['밥', '을', '먹', '고', '학교', '에', '가', 'ㄴ다', '.', '학원', '에', '가', 'ㄴ다', '.']\n",
            "[('밥', 'NNG'), ('을', 'JKO'), ('먹', 'VV'), ('고', 'EC'), ('학교', 'NNG'), ('에', 'JKB'), ('가', 'VV'), ('ㄴ다', 'EF'), ('.', 'SF'), ('학원', 'NNG'), ('에', 'JKB'), ('가', 'VV'), ('ㄴ다', 'EF'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-efjFzUKAP"
      },
      "source": [
        "# 한나눔 (Hannanum)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJqLYshrUGoH",
        "outputId": "84c70bde-8383-4f8a-c251-edf59e712db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(hannanum.nouns(sentence)) # 명사만 추출\n",
        "print(hannanum.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(hannanum.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['밥', '학교', '학원']\n",
            "['밥', '을', '먹', '고', '학교', '에', '가', 'ㄴ다', '.', '학원', '에', '가', 'ㄴ다', '.']\n",
            "[('밥', 'N'), ('을', 'J'), ('먹', 'P'), ('고', 'E'), ('학교', 'N'), ('에', 'J'), ('가', 'P'), ('ㄴ다', 'E'), ('.', 'S'), ('학원', 'N'), ('에', 'J'), ('가', 'P'), ('ㄴ다', 'E'), ('.', 'S')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uRq-geXUYcz"
      },
      "source": [
        "# 메캅 (Mecab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnWIywIsUP7s",
        "outputId": "f63b8c5e-2c6f-4788-cc3a-662a8cdd9888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(mecab.nouns(sentence)) # 명사만 추출\n",
        "print(mecab.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(mecab.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['밥', '학교', '학원']\n",
            "['밥', '을', '먹', '고', '학교', '에', '간다', '.', '학원', '에', '간다', '.']\n",
            "[('밥', 'NNG'), ('을', 'JKO'), ('먹', 'VV'), ('고', 'EC'), ('학교', 'NNG'), ('에', 'JKB'), ('간다', 'VV+EF'), ('.', 'SF'), ('학원', 'NNG'), ('에', 'JKB'), ('간다', 'VV+EF'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkeDjL0KUeRb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}